{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmN2s+dbEO764L8Srnrqpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1998x-stack/Colab/blob/main/clcindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fake-useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3upyokjdCPaj",
        "outputId": "7dedbe65-9e83-4ac0-dd94-e21235b6c5f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (2.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq5fwLss_zrr",
        "outputId": "5da6f392-4893-4cb3-8e9f-6ca2139c52b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to scrape from https://www.clcindex.com/category/Starting to scrape from https://www.clcindex.com/category/\n",
            "\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Starting to scrape from https://www.clcindex.com/category/\n",
            "Data saved to category_data.json at page 20\n",
            "Data saved to category_data.json at page 40\n",
            "Data saved to category_data.json at page 60\n",
            "Data saved to category_data.json at page 80\n",
            "Data saved to category_data.json at page 100\n",
            "Data saved to category_data.json at page 120\n",
            "Data saved to category_data.json at page 140\n",
            "Data saved to category_data.json at page 161\n",
            "Data saved to category_data.json at page 180\n",
            "Data saved to category_data.json at page 200\n",
            "Data saved to category_data.json at page 220\n",
            "Data saved to category_data.json at page 240\n",
            "Data saved to category_data.json at page 260\n",
            "Data saved to category_data.json at page 280\n",
            "Data saved to category_data.json at page 300\n",
            "Data saved to category_data.json at page 320\n",
            "Data saved to category_data.json at page 340\n",
            "Data saved to category_data.json at page 360\n",
            "Data saved to category_data.json at page 380\n",
            "Data saved to category_data.json at page 400\n",
            "Data saved to category_data.json at page 420\n",
            "Data saved to category_data.json at page 440\n",
            "Data saved to category_data.json at page 460\n",
            "Data saved to category_data.json at page 480\n",
            "Data saved to category_data.json at page 500\n",
            "Data saved to category_data.json at page 520\n",
            "Data saved to category_data.json at page 540\n",
            "Data saved to category_data.json at page 560\n",
            "Data saved to category_data.json at page 580\n",
            "Data saved to category_data.json at page 600\n",
            "Data saved to category_data.json at page 620\n",
            "Data saved to category_data.json at page 640\n",
            "Data saved to category_data.json at page 660\n",
            "Data saved to category_data.json at page 680\n",
            "Data saved to category_data.json at page 700\n",
            "Data saved to category_data.json at page 720\n",
            "Data saved to category_data.json at page 740\n",
            "Data saved to category_data.json at page 760\n",
            "Data saved to category_data.json at page 780\n",
            "Data saved to category_data.json at page 800\n",
            "Data saved to category_data.json at page 820\n",
            "Data saved to category_data.json at page 840\n",
            "Data saved to category_data.json at page 860\n",
            "Data saved to category_data.json at page 880\n",
            "Data saved to category_data.json at page 900\n",
            "Data saved to category_data.json at page 920\n",
            "Data saved to category_data.json at page 940\n",
            "Data saved to category_data.json at page 960\n",
            "Data saved to category_data.json at page 980\n",
            "Data saved to category_data.json at page 1000\n",
            "Data saved to category_data.json at page 1020\n",
            "Data saved to category_data.json at page 1040\n",
            "Data saved to category_data.json at page 1060\n",
            "Data saved to category_data.json at page 1080\n",
            "Data saved to category_data.json at page 1100\n",
            "Data saved to category_data.json at page 1120\n",
            "Data saved to category_data.json at page 1140\n",
            "Data saved to category_data.json at page 1160\n",
            "Data saved to category_data.json at page 1180\n",
            "Data saved to category_data.json at page 1200\n",
            "Data saved to category_data.json at page 1220\n",
            "Data saved to category_data.json at page 1240\n",
            "Data saved to category_data.json at page 1260\n",
            "Data saved to category_data.json at page 1280\n",
            "Data saved to category_data.json at page 1300\n",
            "Data saved to category_data.json at page 1320\n",
            "Data saved to category_data.json at page 1340\n",
            "Data saved to category_data.json at page 1360\n",
            "Data saved to category_data.json at page 1380\n",
            "Data saved to category_data.json at page 1400\n",
            "Data saved to category_data.json at page 1420\n",
            "Data saved to category_data.json at page 1440\n",
            "Data saved to category_data.json at page 1460\n",
            "Data saved to category_data.json at page 1480\n",
            "Data saved to category_data.json at page 1500\n",
            "Data saved to category_data.json at page 1520\n",
            "Data saved to category_data.json at page 1540\n",
            "Data saved to category_data.json at page 1560\n",
            "Data saved to category_data.json at page 1580\n",
            "Data saved to category_data.json at page 1600\n",
            "Data saved to category_data.json at page 1620\n",
            "Data saved to category_data.json at page 1640\n",
            "Data saved to category_data.json at page 1660\n",
            "Data saved to category_data.json at page 1680\n",
            "Data saved to category_data.json at page 1700\n",
            "Data saved to category_data.json at page 1720\n",
            "Data saved to category_data.json at page 1740\n",
            "Data saved to category_data.json at page 1760\n",
            "Data saved to category_data.json at page 1780\n",
            "Data saved to category_data.json at page 1800\n",
            "Data saved to category_data.json at page 1820\n",
            "Data saved to category_data.json at page 1840\n",
            "Data saved to category_data.json at page 1860\n",
            "Data saved to category_data.json at page 1880\n",
            "Data saved to category_data.json at page 1900\n",
            "Scraping completed!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "class CategoryScraper:\n",
        "    \"\"\"\n",
        "    A class to scrape categories from the CLC Index website using multi-threading.\n",
        "\n",
        "    Attributes:\n",
        "        base_url (str): The base URL of the CLC Index website.\n",
        "        data (dict): The dictionary to store scraped category data.\n",
        "        visited (set): A set to keep track of already visited category numbers.\n",
        "        save_interval (int): The number of pages to scrape before saving data to file.\n",
        "        ua (UserAgent): Fake User-Agent instance to randomize headers for requests.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_url: str = 'https://www.clcindex.com/category/', save_interval: int = 20, max_retries: int = 5):\n",
        "        \"\"\"\n",
        "        Initializes the CategoryScraper class with the base URL and save interval.\n",
        "\n",
        "        Args:\n",
        "            base_url (str): The base URL of the CLC Index website.\n",
        "            save_interval (int): The number of pages to scrape before saving data.\n",
        "            max_retries (int): The number of retry attempts in case of an error.\n",
        "        \"\"\"\n",
        "        self.base_url = base_url\n",
        "        self.data = {}\n",
        "        self.visited = set()\n",
        "        self.save_interval = save_interval\n",
        "        self.page_counter = 0\n",
        "        self.ua = UserAgent()  # Create a UserAgent instance\n",
        "        self.max_retries = max_retries\n",
        "\n",
        "    def save_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Saves the current scraped data to a local JSON file every save_interval pages.\n",
        "        \"\"\"\n",
        "        filename = 'category_data.json'\n",
        "        # Sort the dictionary by key before saving\n",
        "        sorted_data = {key: self.data[key] for key in sorted(self.data)}\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(sorted_data, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Data saved to {filename} at page {self.page_counter}\")\n",
        "\n",
        "    def scrape_page(self, url: str) -> None:\n",
        "        \"\"\"\n",
        "        Scrapes category data from a specific URL and recursively visits the next page.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL of the page to scrape.\n",
        "        \"\"\"\n",
        "        retries = 0\n",
        "        while retries < self.max_retries:\n",
        "            try:\n",
        "                # Prepare headers with a random User-Agent string\n",
        "                headers = {\n",
        "                    'User-Agent': self.ua.random\n",
        "                }\n",
        "\n",
        "                # Send GET request with the headers\n",
        "                response = requests.get(url, headers=headers)\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Extract category numbers and names\n",
        "                rows = soup.select('table tbody tr')  # Get rows from table\n",
        "                for row in rows:\n",
        "                    category_number = row.select('td:nth-child(2)')  # Get category number\n",
        "                    category_name = row.select('td:nth-child(3)')    # Get category name\n",
        "\n",
        "                    # Ensure category number and name are extracted\n",
        "                    if category_number and category_name:\n",
        "                        cat_number = category_number[0].text.strip()\n",
        "                        cat_name = category_name[0].text.strip()\n",
        "\n",
        "                        # Ensure the category number is not visited before\n",
        "                        if cat_number not in self.visited:\n",
        "                            self.visited.add(cat_number)\n",
        "                            # Add to the data dictionary\n",
        "                            if cat_number not in self.data:\n",
        "                                self.data[cat_number] = [cat_name]\n",
        "\n",
        "                            # Update page counter\n",
        "                            self.page_counter += 1\n",
        "                            if self.page_counter % self.save_interval == 0:\n",
        "                                self.save_data()  # Save data every 20 pages\n",
        "\n",
        "                            # Recursively scrape the next page\n",
        "                            next_page_link = f\"{self.base_url}{cat_number}/\"\n",
        "                            self.scrape_page(next_page_link)\n",
        "\n",
        "                break  # If the scraping succeeds, exit the retry loop\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                print(f\"Error while scraping {url}: {e}. Retry attempt {retries}/{self.max_retries}\")\n",
        "                time.sleep(2 ** retries)  # Exponential backoff before retrying\n",
        "\n",
        "    def run(self, start_url: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Starts the scraping process from the given start URL.\n",
        "\n",
        "        Args:\n",
        "            start_url (str): The starting URL for scraping.\n",
        "        \"\"\"\n",
        "        if start_url is None:\n",
        "            start_url = self.base_url  # Default start URL if not provided\n",
        "        print(f\"Starting to scrape from {start_url}\")\n",
        "        self.scrape_page(start_url)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    Main function to execute the category scraping process using multithreading.\n",
        "    \"\"\"\n",
        "    scraper = CategoryScraper()\n",
        "\n",
        "    # Use ThreadPoolExecutor to speed up the scraping process\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "\n",
        "        # Start multiple threads for scraping pages\n",
        "        for _ in range(10):  # Start 10 threads\n",
        "            futures.append(executor.submit(scraper.run))\n",
        "\n",
        "        # Wait for all threads to complete\n",
        "        for future in as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    scraper.save_data()\n",
        "    print(\"Scraping completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwZ_eHxI_07w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}